{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29948443",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import deeplay as dl\n",
    "import torchvision\n",
    "import PIL\n",
    "#from torchvision import transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import sklearn \n",
    "import os\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dae315",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_mnist_txt(file_path):\n",
    "    \"\"\"Read MNIST-like data from text file\"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            parts = line.strip().split()\n",
    "            image_num = parts[0].strip('\"')\n",
    "            label = int(parts[1])\n",
    "            \n",
    "            # Validate label range\n",
    "            if label < -9 or label > 9:\n",
    "                raise ValueError(f\"Invalid label {label} found in the dataset.\")\n",
    "            \n",
    "            pixels = list(map(float, parts[2:]))\n",
    "            pixel_array = np.array(pixels).reshape(16, 16)\n",
    "            images.append(pixel_array)\n",
    "            labels.append(label)\n",
    "    \n",
    "    return np.array(images), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e4732d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nils\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "C:\\Users\\Nils\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "\n",
      "  | Name          | Type             | Params | Mode \n",
      "-----------------------------------------------------------\n",
      "0 | loss          | CrossEntropyLoss | 0      | train\n",
      "1 | train_metrics | MetricCollection | 0      | train\n",
      "2 | val_metrics   | MetricCollection | 0      | train\n",
      "3 | test_metrics  | MetricCollection | 0      | train\n",
      "4 | model         | Sequential       | 19.7 K | train\n",
      "5 | loss_fn       | NLLLoss          | 0      | train\n",
      "6 | optimizer     | RMSprop          | 0      | train\n",
      "-----------------------------------------------------------\n",
      "19.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "19.7 K    Total params\n",
      "0.079     Total estimated model params size (MB)\n",
      "31        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "C:\\Users\\Nils\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=5` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c7820ea92cb43cb94d9257869e73b99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 test predictions: [-2, 0, -9, 8, 0, 2, -2, 8, 6, 8]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class MNISTDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for MNIST-like data\"\"\"\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Convert to PIL Image for compatibility with torchvision transforms\n",
    "        image = Image.fromarray(image.astype('float32'))\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Shift labels to zero-based indexing\n",
    "        label = torch.tensor(label + 9, dtype=torch.long)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "def create_cnn_model():\n",
    "    \"\"\"Create CNN model using deeplay\"\"\"\n",
    "    conv_base = dl.ConvolutionalNeuralNetwork(\n",
    "        in_channels=1, \n",
    "        hidden_channels=[16, 16, 32], \n",
    "        out_channels=32,\n",
    "    )\n",
    "    conv_base.blocks[2].pool.configure(torch.nn.MaxPool2d, kernel_size=2)\n",
    "\n",
    "    connector = dl.Layer(torch.nn.AdaptiveAvgPool2d, output_size=1)\n",
    "\n",
    "    # Update out_features to 19 for labels -9 to 9\n",
    "    dense_top = dl.MultiLayerPerceptron(\n",
    "        in_features=32,\n",
    "        hidden_features=[64],\n",
    "        out_features=19,  # Updated\n",
    "        out_activation=torch.nn.LogSoftmax(dim=1)\n",
    "    )\n",
    "\n",
    "    return dl.Sequential(conv_base, connector, dense_top)\n",
    "\n",
    "def train_model(train_images, train_labels):\n",
    "    \"\"\"Train the CNN model and return the trained classifier.\"\"\"\n",
    "    # Define transforms\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "\n",
    "    # Create dataset and dataloader\n",
    "    train_dataset = MNISTDataset(train_images, train_labels, transform=transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    # Create model\n",
    "    cnn = create_cnn_model()\n",
    "\n",
    "    # Define classifier\n",
    "    class MNISTClassifier(dl.Classifier):\n",
    "        def __init__(self, model, optimizer):\n",
    "            super().__init__(model=model, optimizer=optimizer)\n",
    "            self.loss_fn = torch.nn.NLLLoss()\n",
    "        \n",
    "        def training_step(self, batch, batch_idx):\n",
    "            x, y = batch\n",
    "            y_hat = self.model(x)\n",
    "            loss = self.loss_fn(y_hat, y)\n",
    "            return loss\n",
    "\n",
    "    classifier = MNISTClassifier(\n",
    "        model=cnn,\n",
    "        optimizer=dl.RMSprop(lr=0.001)\n",
    "    ).create()\n",
    "\n",
    "    # Train the model\n",
    "    trainer = dl.Trainer(max_epochs=20, accelerator=\"auto\")\n",
    "    trainer.fit(classifier, train_loader)\n",
    "\n",
    "    return classifier\n",
    "\n",
    "\n",
    "# Prediction function\n",
    "def predict_with_model(classifier, test_images):\n",
    "    \"\"\"Use trained classifier to make predictions on test_images.\"\"\"\n",
    "    # Define transforms\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "\n",
    "    # Dummy labels for test set\n",
    "    test_dataset = MNISTDataset(test_images, np.zeros(len(test_images)), transform=transform)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Get predictions\n",
    "    test_labels = []\n",
    "    classifier.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, _ in test_loader:\n",
    "            y_hat = classifier.model(x)\n",
    "            preds = torch.argmax(y_hat, dim=1)\n",
    "            test_labels.extend((preds - 9).cpu().numpy().tolist())  # Adjust if needed\n",
    "\n",
    "    return test_labels\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    images, labels = read_mnist_txt(\"Numbers.txt\")\n",
    "\n",
    "    train_images, test_images, train_labels, _ = train_test_split(\n",
    "        images, labels, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    classifier = train_model(train_images, train_labels)\n",
    "    predictions = predict_with_model(classifier, test_images)\n",
    "\n",
    "    print(\"First 10 test predictions:\", predictions[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a034410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training set: (1800, 16, 16)\n",
      "Shape of test set: (200, 16, 16)\n",
      "Trainset shape post transform: (1800, 256)\n",
      "Testset shape posst transform: (200, 256)\n",
      "Number of components before fit (1800, 256)\n",
      "Number of Components post fit: 86\n",
      "Training score: 0.93\n",
      "Testing score 0.905\n",
      "Shape prediction list (200,)\n",
      "[ 5  0  2  0  2  5 -2 -9  1  6  5  0 -9 -2 -9  0  9 -9  9 -9  0  1  8  0\n",
      "  1  2  5  0 -2  0  2  0  0  9  6  1  2  0 -2  8  6  0  6  1  6  2  2  9\n",
      " -2 -2 -2  1  0  9  5 -9  1  0  6  5  6  0  9  5  9  0  8  9  0 -2  5  1\n",
      "  0  9  1  9  0 -9  0 -2  5  0 -2  2 -9  2  9  1  1  9  2  6 -2 -2 -2  0\n",
      " -9  6  0  9  8 -9  6  0  0 -9 -2  6 -2  0 -2  6  1  1  6  8  8  8  5  2\n",
      "  2  0  5  5  1  5  1  6 -2  9  1  1  8  6  0 -9  1  0  6  5 -2  0  2  5\n",
      "  8  0  2 -2  2 -9  5  8 -2  1  1  5  8  0  2  0  2 -2  6  8  5  8  8  1\n",
      "  0  2 -2  0  5  6  1  5 -2  9  1  2  1  0  2 -2  2  6 -2  8  1  8  6  1\n",
      "  1  5 -2  5  8  5  9  5]\n"
     ]
    }
   ],
   "source": [
    "#PCA neural network\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import os\n",
    "\n",
    "def dataPrep(images, labels):\n",
    "    randomIndices = np.random.permutation(len(images))\n",
    "    images = np.array(images)[randomIndices]\n",
    "    labels = np.array(labels)[randomIndices]\n",
    "\n",
    "    trainingSet = images[:len(images)-200]\n",
    "    trainingLabels = labels[:len(images)-200]\n",
    "    testSet = images[-200:]\n",
    "    testSetLabels = labels[-200:]\n",
    "    print('Shape of training set:', trainingSet.shape)\n",
    "    print('Shape of test set:', testSet.shape)\n",
    "    \n",
    "\n",
    "\n",
    "    return trainingSet, trainingLabels, testSet, testSetLabels\n",
    "\n",
    "\n",
    "def dimRedPCA(trainingSet, testSet, trainVariance = .95):\n",
    "    # Reshape for PCA-algorithm\n",
    "    xTrain = trainingSet.reshape(1800, 256)\n",
    "    xTest = testSet.reshape(200, 256)\n",
    "    print('Trainset shape post transform:', xTrain.shape)\n",
    "    print('Testset shape posst transform:', xTest.shape)\n",
    "     # How much of the original variance is retained through the PCA\n",
    "    pca = PCA(n_components=trainVariance)\n",
    "    pca.fit(xTrain) # fit the PCA to the training data\n",
    "    print('Number of components before fit', xTrain.shape)\n",
    "    print('Number of Components post fit:', pca.n_components_)\n",
    "    \n",
    "    trainPCA = pca.transform(xTrain)\n",
    "    testPCA = pca.transform(xTest)\n",
    "\n",
    "    return trainPCA, testPCA\n",
    "    \n",
    "\n",
    "def dataNNClassifier(trainingLabels, trainingData, testLabels, testData):\n",
    "    clf = MLPClassifier(solver='sgd', activation='logistic', max_iter=5000) # sgd = stochastic gradient descent, logistic function for the hidden layer, max_iter = maximum number of itteration, but will conclude after 10 epocs without improvement\n",
    "    clf.fit(trainingData, trainingLabels)\n",
    "    print('Training score:', clf.score(trainingData, trainingLabels))\n",
    "    print('Testing score', clf.score(testData, testLabels))\n",
    "    predictedLabels = clf.predict(testData)\n",
    "    print('Shape prediction list', predictedLabels.shape)\n",
    "    return predictedLabels\n",
    "    \n",
    "def logisticNNPCA(filePath ='Numbers.txt', trainVariance = .95):\n",
    "    # first arg defines the data set\n",
    "    # Second arg sets the variance for the PCA reduction\n",
    "    images, labels = read_mnist_txt(filePath)\n",
    "\n",
    "    trainingSet, trainingLabels, testSet, testSetLabels = dataPrep(images, labels)\n",
    "\n",
    "    trainPCA, testPCA = dimRedPCA(trainingSet, testSet, trainVariance)\n",
    "\n",
    "    predictedLabels = dataNNClassifier(trainingLabels, trainPCA, testSetLabels, testPCA)\n",
    "    print(predictedLabels)\n",
    "    return predictedLabels\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logisticNNPCA()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59856e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Git\\MVE441\\project 1\n",
      "Accuracy: 95.00%\n"
     ]
    }
   ],
   "source": [
    "#random forest classifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def randomForestClassifier(trainingSet, trainingLabels, testSet, n_estimators = 100):\n",
    "    # Flatten 16x16 images to 256-length vectors for classification\n",
    "    X_train = [img.flatten() for img in trainingSet]\n",
    "    X_test = [img.flatten() for img in testSet]\n",
    "    \n",
    "    # Initialize Random Forest\n",
    "    # n_estimators: number of trees in the forest\n",
    "    # random_state: random speed for reproducibility\n",
    "    rf = RandomForestClassifier(n_estimators)\n",
    "    \n",
    "    # Train the Classifier\n",
    "    rf.fit(X_train, trainingLabels)\n",
    "    \n",
    "    # Predict using trained classifier on testSet\n",
    "    predictedLabels = rf.predict(X_test)\n",
    "\n",
    "    return predictedLabels\n",
    "\n",
    "# Load and prepare data\n",
    "print(os.getcwd())\n",
    "images, labels = read_mnist_txt('Numbers.txt')\n",
    "\n",
    "randomIndices = np.random.permutation(len(images))\n",
    "images = np.array(images)[randomIndices]\n",
    "labels = np.array(labels)[randomIndices]\n",
    "\n",
    "trainingSet = images[:len(images)-200]\n",
    "trainingLabels = labels[:len(images)-200]\n",
    "testSet = images[-200:]\n",
    "testSetLabels = labels[-200:]\n",
    "\n",
    "# Call the classifier, no tuning\n",
    "predictedLabels = randomForestClassifier(trainingSet, trainingLabels, testSet, n_estimators=100)\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(testSetLabels, predictedLabels) * 100\n",
    "print(f'Accuracy: {accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e15677",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def cross_validate(images, labels, train_fn, predict_fn, k_folds=5, metric_fn=accuracy_score):\n",
    "    \"\"\"\n",
    "    Generic cross-validation.\n",
    "\n",
    "    Args:\n",
    "        images (np.ndarray): Image data.\n",
    "        labels (np.ndarray): Corresponding labels.\n",
    "        train_fn (callable): Function to train a model. Signature: (train_images, train_labels) -> model\n",
    "        predict_fn (callable): Function to predict. Signature: (model, test_images) -> predictions\n",
    "        k_folds (int): Number of folds (default 5).\n",
    "        metric_fn (callable): Evaluation metric function. Signature: (true_labels, predicted_labels) -> float\n",
    "\n",
    "    Returns:\n",
    "        List of scores for each fold.\n",
    "    \"\"\"\n",
    "    all_scores = []\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(images)):\n",
    "        print(f\"\\nFold {fold + 1}/{k_folds}\")\n",
    "\n",
    "        train_images, val_images = images[train_idx], images[val_idx]\n",
    "        train_labels, val_labels = labels[train_idx], labels[val_idx]\n",
    "\n",
    "        # Train and predict\n",
    "        model = train_fn(train_images, train_labels)\n",
    "        predictions = predict_fn(model, val_images)\n",
    "\n",
    "        # Score\n",
    "        score = metric_fn(val_labels, predictions)\n",
    "        print(f\"Fold {fold + 1} Score: {score:.4f}\")\n",
    "        all_scores.append(score)\n",
    "\n",
    "    print(\"\\nCross-validation results:\")\n",
    "    print(f\"Mean score: {np.mean(all_scores):.4f}\")\n",
    "    print(f\"Std deviation: {np.std(all_scores):.4f}\")\n",
    "\n",
    "    return all_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fa9ae23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Training\" function for k-NN (just returns training data and labels)\n",
    "def train_knn_model(train_images, train_labels, k=3, norm=2):\n",
    "    return {\n",
    "        'train_images': train_images,\n",
    "        'train_labels': train_labels,\n",
    "        'k': k,\n",
    "        'norm': norm\n",
    "    }\n",
    "\n",
    "# Prediction function for k-NN\n",
    "def predict_knn(model, test_images):\n",
    "    train_images = model['train_images']\n",
    "    train_labels = model['train_labels']\n",
    "    k = model['k']\n",
    "    norm = model['norm']\n",
    "    \n",
    "    predictions = []\n",
    "    for image in test_images:\n",
    "        distances = [(\n",
    "            np.linalg.norm(train_image - image, ord=norm), label\n",
    "        ) for train_image, label in zip(train_images, train_labels)]\n",
    "        \n",
    "        neighbors = sorted(distances, key=lambda x: x[0])[:k]\n",
    "        labels = [label for _, label in neighbors]\n",
    "        predicted = max(labels, key=labels.count)\n",
    "        predictions.append(predicted)\n",
    "    \n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "744f55aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1/5\n",
      "Fold 1 Score: 0.8950\n",
      "\n",
      "Fold 2/5\n",
      "Fold 2 Score: 0.9300\n",
      "\n",
      "Fold 3/5\n",
      "Fold 3 Score: 0.9225\n",
      "\n",
      "Fold 4/5\n",
      "Fold 4 Score: 0.9250\n",
      "\n",
      "Fold 5/5\n",
      "Fold 5 Score: 0.9175\n",
      "\n",
      "Cross-validation results:\n",
      "Mean score: 0.9180\n",
      "Std deviation: 0.0122\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    images, labels = read_mnist_txt(\"Numbers.txt\")\n",
    "\n",
    "    # Normalize if needed\n",
    "    flat_images = images.reshape(len(images), -1)\n",
    "    scaler = StandardScaler()\n",
    "    flat_images = scaler.fit_transform(flat_images)\n",
    "\n",
    "    # Create partial functions for fixed k and norm\n",
    "    from functools import partial\n",
    "    k = 5\n",
    "    norm = 2\n",
    "    train_fn = partial(train_knn_model, k=k, norm=norm)\n",
    "\n",
    "    scores = cross_validate(\n",
    "        images=flat_images,\n",
    "        labels=labels,\n",
    "        train_fn=train_fn,\n",
    "        predict_fn=predict_knn,\n",
    "        k_folds=5,\n",
    "        metric_fn=accuracy_score\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5af08257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nils\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "\n",
      "  | Name          | Type             | Params | Mode \n",
      "-----------------------------------------------------------\n",
      "0 | loss          | CrossEntropyLoss | 0      | eval \n",
      "1 | train_metrics | MetricCollection | 0      | train\n",
      "2 | val_metrics   | MetricCollection | 0      | train\n",
      "3 | test_metrics  | MetricCollection | 0      | train\n",
      "4 | model         | Sequential       | 19.7 K | train\n",
      "5 | loss_fn       | NLLLoss          | 0      | train\n",
      "6 | optimizer     | RMSprop          | 0      | train\n",
      "-----------------------------------------------------------\n",
      "19.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "19.7 K    Total params\n",
      "0.079     Total estimated model params size (MB)\n",
      "30        Modules in train mode\n",
      "1         Modules in eval mode\n",
      "C:\\Users\\Nils\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=5` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6805e6ec766c4368b4f807f590445fb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Score: 0.8750\n",
      "\n",
      "Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nils\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "\n",
      "  | Name          | Type             | Params | Mode \n",
      "-----------------------------------------------------------\n",
      "0 | loss          | CrossEntropyLoss | 0      | eval \n",
      "1 | train_metrics | MetricCollection | 0      | train\n",
      "2 | val_metrics   | MetricCollection | 0      | train\n",
      "3 | test_metrics  | MetricCollection | 0      | train\n",
      "4 | model         | Sequential       | 19.7 K | train\n",
      "5 | loss_fn       | NLLLoss          | 0      | train\n",
      "6 | optimizer     | RMSprop          | 0      | train\n",
      "-----------------------------------------------------------\n",
      "19.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "19.7 K    Total params\n",
      "0.079     Total estimated model params size (MB)\n",
      "30        Modules in train mode\n",
      "1         Modules in eval mode\n",
      "C:\\Users\\Nils\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=5` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c59cee95af949f8a2556848576066d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nils\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "\n",
      "  | Name          | Type             | Params | Mode \n",
      "-----------------------------------------------------------\n",
      "0 | loss          | CrossEntropyLoss | 0      | eval \n",
      "1 | train_metrics | MetricCollection | 0      | train\n",
      "2 | val_metrics   | MetricCollection | 0      | train\n",
      "3 | test_metrics  | MetricCollection | 0      | train\n",
      "4 | model         | Sequential       | 19.7 K | train\n",
      "5 | loss_fn       | NLLLoss          | 0      | train\n",
      "6 | optimizer     | RMSprop          | 0      | train\n",
      "-----------------------------------------------------------\n",
      "19.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "19.7 K    Total params\n",
      "0.079     Total estimated model params size (MB)\n",
      "30        Modules in train mode\n",
      "1         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Score: 0.9075\n",
      "\n",
      "Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nils\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=5` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "195ef4566fb6453492011b5c86fc32f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nils\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "\n",
      "  | Name          | Type             | Params | Mode \n",
      "-----------------------------------------------------------\n",
      "0 | loss          | CrossEntropyLoss | 0      | eval \n",
      "1 | train_metrics | MetricCollection | 0      | train\n",
      "2 | val_metrics   | MetricCollection | 0      | train\n",
      "3 | test_metrics  | MetricCollection | 0      | train\n",
      "4 | model         | Sequential       | 19.7 K | train\n",
      "5 | loss_fn       | NLLLoss          | 0      | train\n",
      "6 | optimizer     | RMSprop          | 0      | train\n",
      "-----------------------------------------------------------\n",
      "19.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "19.7 K    Total params\n",
      "0.079     Total estimated model params size (MB)\n",
      "30        Modules in train mode\n",
      "1         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Score: 0.9000\n",
      "\n",
      "Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nils\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=5` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbd5b50aeaa5441dabed91649c4f6774",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nils\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "\n",
      "  | Name          | Type             | Params | Mode \n",
      "-----------------------------------------------------------\n",
      "0 | loss          | CrossEntropyLoss | 0      | eval \n",
      "1 | train_metrics | MetricCollection | 0      | train\n",
      "2 | val_metrics   | MetricCollection | 0      | train\n",
      "3 | test_metrics  | MetricCollection | 0      | train\n",
      "4 | model         | Sequential       | 19.7 K | train\n",
      "5 | loss_fn       | NLLLoss          | 0      | train\n",
      "6 | optimizer     | RMSprop          | 0      | train\n",
      "-----------------------------------------------------------\n",
      "19.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "19.7 K    Total params\n",
      "0.079     Total estimated model params size (MB)\n",
      "30        Modules in train mode\n",
      "1         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Score: 0.8875\n",
      "\n",
      "Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nils\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=5` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab4d91be4cc34fd1a928b07b91bce448",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Score: 0.9025\n",
      "\n",
      "Cross-validation results:\n",
      "Mean score: 0.8945\n",
      "Std deviation: 0.0118\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    images, labels = read_mnist_txt(\"Numbers.txt\")\n",
    "\n",
    "    scores = cross_validate(\n",
    "        images=images,\n",
    "        labels=labels,\n",
    "        train_fn=train_model,\n",
    "        predict_fn=predict_with_model,\n",
    "        k_folds=5,\n",
    "        metric_fn=accuracy_score  # Optional, default already\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
